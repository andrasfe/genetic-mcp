# =============================================================================
# GENETIC MCP CONFIGURATION
# =============================================================================
# This file documents all available environment variables for the genetic-mcp
# server. Copy this to .env and set your values.
#
# Each variable includes:
# - Description of what it controls
# - Where it's used in the code
# - Valid values/examples
# - Default behavior if not set
# =============================================================================

# -----------------------------------------------------------------------------
# TRANSPORT & SERVER CONFIGURATION
# -----------------------------------------------------------------------------

# GENETIC_MCP_TRANSPORT
# Controls the communication protocol for the MCP server
# Used in: genetic_mcp/server.py (line 1129)
# Values: "stdio" (default) | "http"
# - stdio: Standard input/output (for MCP clients like Claude Desktop)
# - http: HTTP server with SSE (for web-based clients)
GENETIC_MCP_TRANSPORT=stdio

# GENETIC_MCP_HOST
# HTTP server bind address (only used when GENETIC_MCP_TRANSPORT=http)
# Used in: genetic_mcp/server.py (line 1153)
# Default: 0.0.0.0 (listen on all interfaces)
GENETIC_MCP_HOST=0.0.0.0

# GENETIC_MCP_PORT
# HTTP server port (only used when GENETIC_MCP_TRANSPORT=http)
# Used in: genetic_mcp/server.py (line 1154)
# Default: 3000
GENETIC_MCP_PORT=3000

# -----------------------------------------------------------------------------
# LLM PROVIDER CONFIGURATION (REQUIRED)
# -----------------------------------------------------------------------------

# DEFAULT_PROVIDER
# Primary LLM provider for idea generation, crossover, and mutation
# Used in: genetic_mcp/server.py (line 136)
# Values: "openai" | "anthropic" | "openrouter"
# This determines which API will be called by MultiModelClient
DEFAULT_PROVIDER=openrouter

# MODEL
# Default model identifier for idea generation
# Used in:
#   - genetic_mcp/server.py (line 131) - As fallback for all providers
#   - genetic_mcp/llm_client.py (line 117) - For embedding fallback
# Examples:
#   - OpenRouter: meta-llama/llama-3.2-3b-instruct, anthropic/claude-3.5-sonnet
#   - OpenAI: gpt-4-turbo-preview, gpt-3.5-turbo
#   - Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
MODEL=meta-llama/llama-3.2-3b-instruct

# OPENAI_MODEL (Optional)
# Provider-specific model override for OpenAI
# Used in: genetic_mcp/server.py (line 142)
# If not set, falls back to MODEL
# Examples: gpt-4-turbo-preview, gpt-4, gpt-3.5-turbo
OPENAI_MODEL=

# ANTHROPIC_MODEL (Optional)
# Provider-specific model override for Anthropic
# Used in: genetic_mcp/server.py (line 143)
# If not set, falls back to MODEL
# Examples: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
ANTHROPIC_MODEL=

# OPENROUTER_MODEL (Optional)
# Provider-specific model override for OpenRouter
# Used in: genetic_mcp/server.py (line 144)
# If not set, falls back to MODEL
# Examples: meta-llama/llama-3.2-3b-instruct, anthropic/claude-3.5-sonnet
OPENROUTER_MODEL=

# -----------------------------------------------------------------------------
# API KEYS (At least one required based on DEFAULT_PROVIDER)
# -----------------------------------------------------------------------------

# OPENAI_API_KEY
# OpenAI API key for GPT models
# Used in:
#   - genetic_mcp/server.py (line 127) - For LLM client initialization
#   - genetic_mcp/embedding_providers.py (line 278, 372) - For OpenAI embeddings
# Required if: DEFAULT_PROVIDER=openai OR EMBEDDING_PROVIDER=openai
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# ANTHROPIC_API_KEY
# Anthropic API key for Claude models
# Used in: genetic_mcp/server.py (line 128)
# Required if: DEFAULT_PROVIDER=anthropic
# Get from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OPENROUTER_API_KEY
# OpenRouter API key for multi-model access
# Used in: genetic_mcp/server.py (line 129)
# Required if: DEFAULT_PROVIDER=openrouter
# Get from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# COHERE_API_KEY (Optional)
# Cohere API key for embeddings or additional LLM access
# Used in: genetic_mcp/embedding_providers.py (lines 289, 296)
# Required if: EMBEDDING_PROVIDER=cohere or EMBEDDING_PROVIDER=cohere-v2
# Get from: https://dashboard.cohere.com/api-keys
COHERE_API_KEY=

# VOYAGE_API_KEY (Optional)
# Voyage AI API key for embeddings
# Used in: genetic_mcp/embedding_providers.py (line 306)
# Required if: EMBEDDING_PROVIDER=voyage
# Get from: https://www.voyageai.com/
VOYAGE_API_KEY=

# -----------------------------------------------------------------------------
# SESSION MANAGEMENT
# -----------------------------------------------------------------------------

# SESSION_TIMEOUT_MINUTES
# How long sessions remain in memory after completion/failure
# Used in: genetic_mcp/session_manager.py (line 43)
# Default: 60 minutes
# The cleanup loop (line 549) checks every 5 minutes for expired sessions
SESSION_TIMEOUT_MINUTES=60

# MAX_SESSIONS_PER_CLIENT
# Maximum concurrent sessions allowed per client ID
# Used in: genetic_mcp/session_manager.py (lines 37, 103)
# Default: 10
# Prevents resource exhaustion from single clients
MAX_SESSIONS_PER_CLIENT=10

# -----------------------------------------------------------------------------
# WORKER POOL CONFIGURATION
# -----------------------------------------------------------------------------

# MAX_WORKERS_PER_SESSION
# Maximum parallel LLM workers for idea generation
# Used in: genetic_mcp/session_manager.py (lines 176, 650)
# Default: 20
# More workers = faster generation but higher API costs and rate limits
# Worker pool distributes idea generation tasks across this many workers
MAX_WORKERS_PER_SESSION=20

# WORKER_TIMEOUT_SECONDS
# Timeout for individual LLM API calls
# Used in: genetic_mcp/worker_pool.py (if implemented)
# Default: 120 seconds
# Prevents hanging on slow/failed API calls
WORKER_TIMEOUT_SECONDS=120

# -----------------------------------------------------------------------------
# GENETIC ALGORITHM DEFAULTS
# -----------------------------------------------------------------------------
# These values are used as defaults when creating sessions if not specified
# in the GenerationRequest. They can be overridden per-session via MCP tools.
# Used in: genetic_mcp/models.py (GeneticParameters model)

# DEFAULT_POPULATION_SIZE
# Number of ideas in each generation
# Default: 10
# Larger populations = more diversity but slower and more expensive
DEFAULT_POPULATION_SIZE=10

# DEFAULT_GENERATIONS
# Number of evolutionary iterations to run
# Default: 5
# More generations = more refinement but longer execution time
DEFAULT_GENERATIONS=5

# DEFAULT_MUTATION_RATE
# Probability (0-1) of mutating an idea during evolution
# Default: 0.1 (10%)
# Higher rates = more exploration, lower rates = more exploitation
DEFAULT_MUTATION_RATE=0.1

# DEFAULT_CROSSOVER_RATE
# Probability (0-1) of crossover between parent ideas
# Default: 0.7 (70%)
# Higher rates = more combination of ideas, lower = more preservation
DEFAULT_CROSSOVER_RATE=0.7

# DEFAULT_ELITISM_COUNT
# Number of top ideas preserved unchanged to next generation
# Default: 2
# Ensures best ideas are never lost during evolution
DEFAULT_ELITISM_COUNT=2

# -----------------------------------------------------------------------------
# FITNESS EVALUATION WEIGHTS
# -----------------------------------------------------------------------------
# These control the multi-objective fitness function
# Used in: genetic_mcp/models.py (FitnessWeights model)
# MUST SUM TO 1.0

# FITNESS_WEIGHT_RELEVANCE
# Weight for semantic similarity to the target prompt
# Used in: genetic_mcp/fitness.py (_calculate_relevance)
# Higher = prioritize ideas closer to the original prompt
FITNESS_WEIGHT_RELEVANCE=0.4

# FITNESS_WEIGHT_NOVELTY
# Weight for diversity from other ideas in population
# Used in: genetic_mcp/fitness.py (_calculate_novelty)
# Higher = prioritize unique/diverse ideas
FITNESS_WEIGHT_NOVELTY=0.3

# FITNESS_WEIGHT_FEASIBILITY
# Weight for practical implementation feasibility
# Used in: genetic_mcp/fitness.py (_calculate_feasibility)
# Higher = prioritize actionable, well-structured ideas
FITNESS_WEIGHT_FEASIBILITY=0.3

# -----------------------------------------------------------------------------
# EMBEDDING CONFIGURATION
# -----------------------------------------------------------------------------

# EMBEDDING_PROVIDER
# Provider for semantic embeddings (used in fitness calculation)
# Used in: genetic_mcp/embedding_providers.py (line 338)
# Values: "openai" | "cohere" | "cohere-v2" | "sentence-transformer" | "voyage" | "dummy"
# Automatic fallback chain (line 372):
#   1. sentence-transformer (local, no API key needed)
#   2. openai (if OPENAI_API_KEY is set)
#   3. error
# - sentence-transformer: Local models, no API calls, free
# - openai: text-embedding-ada-002, text-embedding-3-small/large
# - cohere: embed-english-v3.0 and other Cohere models
# - cohere-v2: Newer Cohere API version with input_type parameter
# - voyage: voyage-2 and other Voyage AI models
# - dummy: Random embeddings for testing
EMBEDDING_PROVIDER=sentence-transformer

# EMBEDDING_MODEL
# Specific model for the chosen embedding provider
# Used in:
#   - genetic_mcp/embedding_providers.py (lines 292, 299, 343-349)
#   - genetic_mcp/llm_client.py (line 34, 117)
# Provider-specific defaults:
#   - openai: text-embedding-ada-002
#   - cohere: embed-english-v3.0
#   - cohere-v2: embed-english-v3.0
#   - sentence-transformer: all-MiniLM-L6-v2 (local)
#   - voyage: voyage-2
EMBEDDING_MODEL=all-MiniLM-L6-v2

# EMBEDDING_CACHE_SIZE
# LRU cache size for embeddings (reduces duplicate API calls)
# Used in: genetic_mcp/embedding_providers.py (if implemented with functools.lru_cache)
# Default: 10000
# Larger cache = fewer API calls but more memory usage
EMBEDDING_CACHE_SIZE=10000

# -----------------------------------------------------------------------------
# LOGGING & DEBUGGING
# -----------------------------------------------------------------------------

# GENETIC_MCP_DEBUG
# Enable debug mode with verbose logging
# Used in: genetic_mcp/server.py and logging setup
# Values: "true" | "false"
# When true, sets log level to DEBUG and enables additional logging
GENETIC_MCP_DEBUG=false

# GENETIC_MCP_LOG_LEVEL or LOG_LEVEL
# Logging verbosity level
# Used in: genetic_mcp/server.py (line 32)
# Values: "DEBUG" | "INFO" | "WARNING" | "ERROR"
# - DEBUG: All messages including detailed execution traces
# - INFO: General informational messages (default)
# - WARNING: Warning messages only
# - ERROR: Error messages only
LOG_LEVEL=INFO

# GENETIC_MCP_LOG_FILE or LOG_FILE
# Path to log file (if specified, logs are written to file)
# Used in: genetic_mcp/server.py (line 33)
# If not set, logs go to stderr only
# Example: genetic_mcp.log, /var/log/genetic_mcp.log
LOG_FILE=

# -----------------------------------------------------------------------------
# OPTIMIZATION & PERFORMANCE (Optional)
# -----------------------------------------------------------------------------

# GENETIC_MCP_OPTIMIZATION_ENABLED
# Enable advanced optimization features
# Used in: genetic_mcp/server.py (lines 527, 1105)
# Values: "true" | "false"
# Default: false
GENETIC_MCP_OPTIMIZATION_ENABLED=false

# GENETIC_MCP_OPTIMIZATION_LEVEL
# Optimization level when optimization is enabled
# Used in: genetic_mcp/server.py (line 528)
# Values: "basic" | "aggressive"
# Default: basic
GENETIC_MCP_OPTIMIZATION_LEVEL=basic

# GENETIC_MCP_USE_GPU
# Enable GPU acceleration (requires CUDA and PyTorch)
# Used in: genetic_mcp/server.py (line 529)
# Values: "true" | "false"
# Default: false
# Automatically falls back to CPU if GPU not available
GENETIC_MCP_USE_GPU=false

# -----------------------------------------------------------------------------
# MEMORY & LEARNING SYSTEM (Optional)
# -----------------------------------------------------------------------------

# GENETIC_MCP_MEMORY_ENABLED
# Enable persistent learning across sessions
# Used in: genetic_mcp/memory_system.py
# Values: "true" | "false"
# Default: true
# When enabled, system learns from successful sessions and provides
# parameter recommendations for similar prompts
GENETIC_MCP_MEMORY_ENABLED=true

# GENETIC_MCP_MEMORY_DB
# Path to SQLite database for memory system
# Used in: genetic_mcp/memory_system.py
# Default: genetic_mcp_memory.db
# Stores successful evolution patterns and parameter recommendations
GENETIC_MCP_MEMORY_DB=genetic_mcp_memory.db

# -----------------------------------------------------------------------------
# SECURITY & RATE LIMITING (Optional)
# -----------------------------------------------------------------------------

# MAX_REQUEST_SIZE_MB
# Maximum size of incoming requests in megabytes
# Used in: HTTP transport layer (if implemented)
# Default: 10
# Prevents memory exhaustion from large payloads
MAX_REQUEST_SIZE_MB=10

# RATE_LIMIT_REQUESTS_PER_MINUTE
# Maximum requests per minute per client
# Used in: HTTP transport layer (if implemented)
# Default: 60
# Prevents abuse and ensures fair resource allocation
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# =============================================================================
# PROVIDER-SPECIFIC EMBEDDING MODEL OVERRIDES (Optional)
# =============================================================================
# These allow fine-grained control over embedding models per provider
# Used in: genetic_mcp/embedding_providers.py (lines 343-349)

# OPENAI_EMBEDDING_MODEL
# OpenAI-specific embedding model override
# Falls back to EMBEDDING_MODEL if not set
# Examples: text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large
OPENAI_EMBEDDING_MODEL=

# COHERE_EMBEDDING_MODEL
# Cohere-specific embedding model override
# Falls back to EMBEDDING_MODEL if not set
# Examples: embed-english-v3.0, embed-multilingual-v3.0, embed-english-light-v3.0
COHERE_EMBEDDING_MODEL=

# SENTENCE_TRANSFORMER_EMBEDDING_MODEL
# Sentence Transformers-specific model override
# Falls back to EMBEDDING_MODEL if not set
# Examples: all-MiniLM-L6-v2, all-mpnet-base-v2, multi-qa-MiniLM-L6-cos-v1
SENTENCE_TRANSFORMER_EMBEDDING_MODEL=

# VOYAGE_EMBEDDING_MODEL
# Voyage AI-specific embedding model override
# Falls back to EMBEDDING_MODEL if not set
# Examples: voyage-2, voyage-large-2, voyage-code-2
VOYAGE_EMBEDDING_MODEL=

# =============================================================================
# QUICK START GUIDE
# =============================================================================
#
# Minimum required configuration:
# 1. Set DEFAULT_PROVIDER (openai, anthropic, or openrouter)
# 2. Set MODEL for your chosen provider
# 3. Set the corresponding API key (OPENAI_API_KEY, ANTHROPIC_API_KEY, or OPENROUTER_API_KEY)
#
# Example for OpenRouter:
#   DEFAULT_PROVIDER=openrouter
#   MODEL=meta-llama/llama-3.2-3b-instruct
#   OPENROUTER_API_KEY=sk-or-v1-xxxxx
#
# Example for OpenAI:
#   DEFAULT_PROVIDER=openai
#   MODEL=gpt-4-turbo-preview
#   OPENAI_API_KEY=sk-xxxxx
#
# For embeddings:
# - If you don't set EMBEDDING_PROVIDER, it will auto-detect:
#   1. Try sentence-transformer (local, free, no API needed)
#   2. Fall back to OpenAI if OPENAI_API_KEY is set
# - For production, set EMBEDDING_PROVIDER explicitly
#
# =============================================================================
